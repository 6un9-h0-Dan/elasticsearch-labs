{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# NLP text search using hugging face transformer model\n",
    "The workbook implements NLP text search in Elasticsearch using a simple dataset consisting of Elastic blogs titles.\n",
    "\n",
    "You will index blogs documents, and using ingest pipeline generate text embeddings. By using NLP model you will query the documents using natural language over the the blogs documents.\n",
    "\n",
    "\n",
    "## Prerequisities\n",
    "Before we begin, create an elastic cloud deployment and [autoscale](https://www.elastic.co/guide/en/cloud/current/ec-autoscaling.html) to have least one machine learning (ML) node with enough (4GB) memory. Also ensure that the Elasticsearch cluster is running. \n",
    "\n",
    "If you don't already have an Elastic deployment, you can sign up for a free [Elastic Cloud trial](https://cloud.elastic.co/registration?fromURI=%2Fhome).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdzl8tmZfr3y"
   },
   "source": [
    "## Install packages and import modules\n",
    "Before you start you need to install all required Python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NM_6fGFURcz",
    "outputId": "53f1a78c-db7f-468e-c6af-bdf9be554473"
   },
   "outputs": [],
   "source": [
    "# install packages\n",
    "!python3 -m pip install -qU sentence-transformers eland elasticsearch transformers multiprocessing\n",
    "\n",
    "# import modules\n",
    "import pandas as pd, json\n",
    "from elasticsearch import Elasticsearch\n",
    "from getpass import getpass\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKU9L8o2FodV"
   },
   "source": [
    "## Deploy an NLP model\n",
    "\n",
    "We are using the [`eland`](https://www.elastic.co/guide/en/elasticsearch/client/eland/current/overview.html) tool to install a `text_embedding` model. For our model, We have used [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) to transform the search text into the dense vector. \n",
    "\n",
    "The model will transfer your search query into vector which will be used for the search over the set of documents stored in Elasticsearch. \n",
    "\n",
    "\n",
    "## Install text embedding NLP model\n",
    "\n",
    "Using the [`eland_import_hub_model`](https://www.elastic.co/guide/en/elasticsearch/client/eland/current/machine-learning.html#ml-nlp-pytorch) script,  download and install `all-MiniLM-L6-v2` transformer model. Setting the NLP `--task-type` as `text_embedding`. \n",
    "\n",
    "To get the cloud id, go to [Elastic cloud](https://cloud.elastic.co) and `On the deployment overview page, copy down the Cloud ID.`\n",
    "\n",
    "To authenticate your request, You could use [API key](https://www.elastic.co/guide/en/kibana/current/api-keys.html#create-api-key). Alternatively, you can use your cloud deployment username and password.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQkKn02j_FfJ",
    "outputId": "99b9ffd4-6780-4167-dbd7-e0264369e40e"
   },
   "outputs": [],
   "source": [
    "API_KEY = getpass(\"Elastic deployment API Key\")\n",
    "CLOUD_ID = getpass(\"Elastic deployment Cloud ID\")\n",
    "!eland_import_hub_model --cloud-id $CLOUD_ID --hub-model-id sentence-transformers/all-MiniLM-L6-v2 --task-type text_embedding --es-api-key $API_KEY --start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGfkUDWDMkc4"
   },
   "source": [
    "## Connect to Elasticsearch cluster\n",
    "\n",
    "Create a elasticsearch client instance with your deployment `Cloud Id` and `API Key`. In this example, we are using the `API_KEY` and `CLOUD_ID` value from previous step. \n",
    "\n",
    "Alternately you could use your deployment `Username` and `Password` to authenticate your instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGi175RbJhVQ",
    "outputId": "14e7f403-06ba-4fa7-a3f4-4bcc271a1f5d"
   },
   "outputs": [],
   "source": [
    "# ELASTIC_CLOUD_USERNAME = getpass(\"Elastic username\")\n",
    "# ELASTIC_CLOUD_PASSWORD = getpass(\"Elastic Password\")\n",
    "\n",
    "es = Elasticsearch(\n",
    "  cloud_id=CLOUD_ID,\n",
    "  #basic_auth=(ELASTIC_CLOUD_USERNAME, ELASTIC_CLOUD_PASSWORD),\n",
    "  api_key=API_KEY,\n",
    "  request_timeout=600\n",
    ")\n",
    "\n",
    "es.info() # should return cluster info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FoZ5TBrIqOT"
   },
   "source": [
    "## Create an Ingest pipeline\n",
    "\n",
    "We need to create a text embedding ingest pipeline to generate vector (text) embeddings for `title` field.\n",
    "\n",
    "The pipeline below is defining a processor for the [inference](https://www.elastic.co/guide/en/elasticsearch/reference/current/inference-processor.html) to the NLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geY7WLh7Ky-k",
    "outputId": "97c17b36-94d6-454f-b976-4c20e8e49edc"
   },
   "outputs": [],
   "source": [
    "# ingest pipeline definition\n",
    "PIPELINE_ID=\"vectorize_blogs\"\n",
    "\n",
    "es.ingest.put_pipeline(id=PIPELINE_ID, processors=[{\n",
    "        \"inference\": {\n",
    "          \"model_id\": \"sentence-transformers__all-minilm-l6-v2\",\n",
    "          \"target_field\": \"text_embedding\",\n",
    "          \"field_map\": {\n",
    "            \"title\": \"text_field\"\n",
    "          }\n",
    "        }\n",
    "      }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IW-GIlH2OxB4"
   },
   "source": [
    "## Create Index with mappings\n",
    "\n",
    "We will now create an elasticsearch index with correct mapping before we index documents. \n",
    "We are adding `text_embedding` to include the `model_id` and `predicted_value` to store the embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xAkc1OVcOxy3",
    "outputId": "b2453634-89b8-48bc-ac65-a6a1c3b8170f"
   },
   "outputs": [],
   "source": [
    "# define index name\n",
    "INDEX_NAME=\"blogs\"\n",
    "\n",
    "# flag to check if index has to be deleted before creating\n",
    "SHOULD_DELETE_INDEX=True\n",
    "\n",
    "# define index mapping\n",
    "INDEX_MAPPING = {\n",
    "    \"properties\": {\n",
    "      \"title\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \n",
    "      \"text_embedding\": {\n",
    "        \"properties\": {\n",
    "          \"is_truncated\": {\n",
    "            \"type\": \"boolean\"\n",
    "          },\n",
    "          \"model_id\": {\n",
    "            \"type\": \"text\",\n",
    "            \"fields\": {\n",
    "              \"keyword\": {\n",
    "                \"type\": \"keyword\",\n",
    "                \"ignore_above\": 256\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"predicted_value\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 384,\n",
    "            \"index\": True,\n",
    "            \"similarity\": \"l2_norm\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "INDEX_SETTINGS = {\n",
    "    \"index\": {\n",
    "      \"number_of_replicas\": \"1\",\n",
    "      \"number_of_shards\": \"1\",\n",
    "      \"default_pipeline\": PIPELINE_ID\n",
    "    }\n",
    "}\n",
    "\n",
    "# check if we want to delete index before creating the index\n",
    "if(SHOULD_DELETE_INDEX):\n",
    "  if es.indices.exists(index=INDEX_NAME):\n",
    "    print(\"Deleting existing %s\" % INDEX_NAME)\n",
    "    es.indices.delete(index=INDEX_NAME, ignore=[400, 404])\n",
    "\n",
    "print(\"Creating index %s\" % INDEX_NAME)\n",
    "es.indices.create(index=INDEX_NAME, mappings=INDEX_MAPPING, settings=INDEX_SETTINGS,\n",
    "                  ignore=[400, 404])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOGsvnGveAoP"
   },
   "source": [
    "## Index data to elasticsearch index\n",
    "\n",
    "Let's index sample blogs data using the ingest pipeline. \n",
    "\n",
    "Note: Before we begin indexing, ensure you have [started your trained model deployment](https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-deploy-model.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/notebooks/hugging-face/blogs.json\"\n",
    "response = urlopen(url)\n",
    "titles = json.loads(response.read())\n",
    "\n",
    "actions = []\n",
    "for title in titles:\n",
    "    actions.append({\"index\": {\"_index\": \"blogs\"}})\n",
    "    actions.append(title)\n",
    "es.bulk(index=\"blogs\", operations=actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPPHg8K8T3wY"
   },
   "source": [
    "## Querying the dataset\n",
    "The next step is to run a query to search for relevant blogs. The example query searches for `\"model_text\": \"how to track network connections\"` using the model we uploaded to Elasticsearch `sentence-transformers__all-minilm-l6-v2`.\n",
    "\n",
    "The process is one query even it internally consists of two tasks. One is to transform your search text into a vector using the NLP model and the second task is to run the vector search over the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 566
    },
    "id": "c4G5V9wmU9C5",
    "outputId": "c8f0cc24-5713-4560-8a5d-c42da562a670"
   },
   "outputs": [],
   "source": [
    "INDEX_NAME=\"blogs\"\n",
    "\n",
    "source_fields = [ \"id\", \"title\"]\n",
    "\n",
    "query = {\n",
    "  \"field\": \"text_embedding.predicted_value\",\n",
    "  \"k\": 10,\n",
    "  \"num_candidates\": 50,\n",
    "  \"query_vector_builder\": {\n",
    "    \"text_embedding\": {\"model_id\": \"sentence-transformers__all-minilm-l6-v2\",\n",
    "      \"model_text\": \"how to track network connections\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = es.search(\n",
    "    index=INDEX_NAME,\n",
    "    fields=source_fields,\n",
    "    knn=query,\n",
    "    source=False)\n",
    "\n",
    "\n",
    "results = pd.json_normalize(json.loads(json.dumps(response.body['hits']['hits'])))\n",
    "\n",
    "# shows the result\n",
    "results[['_id', '_score', 'fields.title']]\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
