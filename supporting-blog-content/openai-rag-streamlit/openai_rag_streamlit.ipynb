{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f96b815e"
      },
      "source": [
        "# Semantic search and Retrieval augmented generation using Elasticsearch and OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0f537af"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elastic/elasticsearch-labs/blob/main/notebooks/integrations/openai/openai-KNN-RAG.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "349e0e74"
      },
      "source": [
        "This notebook demonstrates how to:\n",
        "- Index the OpenAI Wikipedia vector dataset into Elasticsearch\n",
        "- Embed a question with the OpenAI [`embeddings`](https://platform.openai.com/docs/api-reference/embeddings) endpoint\n",
        "- Perform semantic search on the Elasticsearch index using the encoded question\n",
        "- Send the top search results to the OpenAI [Chat Completions](https://platform.openai.com/docs/guides/gpt/chat-completions-api) API endpoint for retrieval augmented generation (RAG)\n",
        "- Build a simple search interface using Streamlit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa9576ca"
      },
      "source": [
        "## Install packages and import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c304b93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59cb24eb-ede6-421c-dc4f-a8f6cdafc2be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/73.6 kB\u001b[0m \u001b[31m965.2 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.5/395.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# install packages\n",
        "\n",
        "!python3 -m pip install -qU openai pandas==1.5.3 wget elasticsearch streamlit tqdm\n",
        "\n",
        "# import modules\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "from elasticsearch import Elasticsearch, helpers\n",
        "import wget, zipfile, pandas as pd, json, openai\n",
        "import streamlit as st\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de32a789"
      },
      "source": [
        "## Connect to Elasticsearch\n",
        "\n",
        "ℹ️ We're using an Elastic Cloud deployment of Elasticsearch for this notebook.\n",
        "If you don't already have an Elastic deployment, you can sign up for a free [Elastic Cloud trial](https://cloud.elastic.co/registration?fromURI=%2Fhome).\n",
        "\n",
        "To connect to Elasticsearch, you need to create a client instance with the Cloud ID and password for your deployment.\n",
        "\n",
        "Find the Cloud ID for your deployment by going to https://cloud.elastic.co/deployments and selecting your deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a57b6a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "edc231d3-eb69-4e0b-f31b-38bf65e653b4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f93760f67190>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'es_cloud_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Elastic deployment Cloud ID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'es_password'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Elastic deployment Password\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'getpass' is not defined"
          ]
        }
      ],
      "source": [
        "os.environ['es_cloud_id'] = getpass(\"Elastic deployment Cloud ID\")\n",
        "os.environ['es_password'] = getpass(\"Elastic deployment Password\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es_cloud_id = os.environ['es_cloud_id']\n",
        "es_password = os.environ['es_password']\n",
        "\n",
        "client = Elasticsearch(\n",
        "  cloud_id = es_cloud_id,\n",
        "  basic_auth=(\"elastic\", es_password) # Alternatively use `api_key` instead of `basic_auth`\n",
        ")\n",
        "\n",
        "# Test connection to Elasticsearch\n",
        "print(client.info())"
      ],
      "metadata": {
        "id": "NJkflYkGHnGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80b55952"
      },
      "source": [
        "## Download the dataset\n",
        "\n",
        "In this step we download the OpenAI Wikipedia embeddings dataset, and extract the zip file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c584f15c"
      },
      "outputs": [],
      "source": [
        "embeddings_url = 'https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip'\n",
        "wget.download(embeddings_url)\n",
        "\n",
        "with zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\n",
        "\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9654ac08"
      },
      "source": [
        "##  Read CSV file into a Pandas DataFrame\n",
        "\n",
        "Next we use the Pandas library to read the unzipped CSV file into a DataFrame. This step makes it easier to index the data into Elasticsearch in bulk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76347d10"
      },
      "outputs": [],
      "source": [
        "\n",
        "wikipedia_dataframe = pd.read_csv(\"data/vector_database_wikipedia_articles_embedded.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6af9f5ad"
      },
      "source": [
        "## Create index with mapping\n",
        "\n",
        "Now we need to create an Elasticsearch index with the necessary mappings. This will enable us to index the data into Elasticsearch.\n",
        "\n",
        "We use the `dense_vector` field type for the `title_vector` and  `content_vector` fields. This is a special field type that allows us to store dense vectors in Elasticsearch.\n",
        "\n",
        "Later, we'll need to target the `dense_vector` field for kNN search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "681989b3"
      },
      "outputs": [],
      "source": [
        "index_mapping= {\n",
        "    \"properties\": {\n",
        "      \"title_vector\": {\n",
        "          \"type\": \"dense_vector\",\n",
        "          \"dims\": 1536,\n",
        "          \"index\": \"true\",\n",
        "          \"similarity\": \"cosine\"\n",
        "      },\n",
        "      \"content_vector\": {\n",
        "          \"type\": \"dense_vector\",\n",
        "          \"dims\": 1536,\n",
        "          \"index\": \"true\",\n",
        "          \"similarity\": \"cosine\"\n",
        "      },\n",
        "      \"text\": {\"type\": \"text\"},\n",
        "      \"title\": {\"type\": \"text\"},\n",
        "      \"url\": { \"type\": \"keyword\"},\n",
        "      \"vector_id\": {\"type\": \"long\"}\n",
        "\n",
        "    }\n",
        "}\n",
        "client.indices.create(index=\"wikipedia_vector_index\", mappings=index_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2fb582e"
      },
      "source": [
        "## Index data into Elasticsearch\n",
        "\n",
        "The following function generates the required bulk actions that can be passed to Elasticsearch's Bulk API, so we can index multiple documents efficiently in a single request.\n",
        "\n",
        "For each row in the DataFrame, the function yields a dictionary representing a single document to be indexed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efee9b97"
      },
      "outputs": [],
      "source": [
        "def dataframe_to_bulk_actions(df):\n",
        "    for index, row in df.iterrows():\n",
        "        yield {\n",
        "            \"_index\": 'wikipedia_vector_index',\n",
        "            \"_id\": row['id'],\n",
        "            \"_source\": {\n",
        "                'url' : row[\"url\"],\n",
        "                'title' : row[\"title\"],\n",
        "                'text' : row[\"text\"],\n",
        "                'title_vector' : json.loads(row[\"title_vector\"]),\n",
        "                'content_vector' : json.loads(row[\"content_vector\"]),\n",
        "                'vector_id' : row[\"vector_id\"]\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8164b38"
      },
      "source": [
        "As the dataframe is large, we will index data in batches of `100`. We index the data into Elasticsearch using the Python client's [helpers](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/client-helpers.html#bulk-helpers) for the bulk API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aacb5e9c"
      },
      "outputs": [],
      "source": [
        "total_documents = len(wikipedia_dataframe)\n",
        "\n",
        "progress_bar = tqdm(total=total_documents, unit=\"documents\")\n",
        "success_count = 0\n",
        "\n",
        "for ok, info in helpers.streaming_bulk(client, actions=dataframe_to_bulk_actions(wikipedia_dataframe), raise_on_error=False, chunk_size=100):\n",
        "  if ok:\n",
        "    success_count += 1\n",
        "  else:\n",
        "    print(f\"Unable to index {info['index']['_id']}: {info['index']['error']}\")\n",
        "  progress_bar.update(1)\n",
        "  progress_bar.set_postfix(success=success_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "091ffc51"
      },
      "source": [
        "Let's test the index with a simple match query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ccc8955"
      },
      "outputs": [],
      "source": [
        "print(client.search(index=\"wikipedia_vector_index\", query={\n",
        "    \"match\": {\n",
        "      \"text\": {\n",
        "        \"query\": \"Hummingbird\"\n",
        "      }\n",
        "    }\n",
        "}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "992b6804"
      },
      "source": [
        "## Encode a question with OpenAI embedding model\n",
        "\n",
        "To perform semantic search, we need to encode queries with the same embedding model used to encode the documents at index time.\n",
        "In this example, we need to use the `text-embedding-ada-002` model.\n",
        "\n",
        "You'll need your OpenAI [API key](https://platform.openai.com/account/api-keys) to generate the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57385c69"
      },
      "outputs": [],
      "source": [
        "# Get OpenAI API key\n",
        "os.environ['openai_api_key'] = getpass(\"Enter OpenAI API key\")\n",
        "\n",
        "# Set API key\n",
        "openai.api_key = os.environ['openai_api_key']\n",
        "\n",
        "# Define model\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "\n",
        "# Define question\n",
        "question = 'How big is the Atlantic ocean?'\n",
        "\n",
        "# Create embedding\n",
        "question_embedding = openai.Embedding.create(input=question, model=EMBEDDING_MODEL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7e6bf5d"
      },
      "source": [
        "## Run semantic search queries\n",
        "\n",
        "Now we're ready to run queries against our Elasticsearch index using our encoded question. We'll be doing a k-nearest neighbors search, using the Elasticsearch [kNN query](https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html) option.\n",
        "\n",
        "First, we define a small function to pretty print the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1291434"
      },
      "outputs": [],
      "source": [
        "# Function to pretty print Elasticsearch results\n",
        "\n",
        "def pretty_response(response):\n",
        "    for hit in response['hits']['hits']:\n",
        "        id = hit['_id']\n",
        "        score = hit['_score']\n",
        "        title = hit['_source']['title']\n",
        "        text = hit['_source']['text']\n",
        "        pretty_output = (f\"\\nID: {id}\\nTitle: {title}\\nSummary: {text}\\nScore: {score}\")\n",
        "        print(pretty_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed8a497f"
      },
      "source": [
        "Now let's run our `kNN` query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc834fdd"
      },
      "outputs": [],
      "source": [
        "response = client.search(\n",
        "  index = \"wikipedia_vector_index\",\n",
        "  knn={\n",
        "      \"field\": \"content_vector\",\n",
        "      \"query_vector\":  question_embedding[\"data\"][0][\"embedding\"],\n",
        "      \"k\": 10,\n",
        "      \"num_candidates\": 100\n",
        "    }\n",
        ")\n",
        "pretty_response(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "276c1147"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "Success! Now you know how to use Elasticsearch as a vector database to store embeddings, encode queries by calling the OpenAI [`embeddings`](https://platform.openai.com/docs/api-reference/embeddings) endpoint, and run semantic search using [kNN search](https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html) to find the top results.\n",
        "\n",
        "Play around with different queries, and if you want to try with your own data, you can experiment with different embedding models.\n",
        "\n",
        "Now we can use the [Chat completions](https://platform.openai.com/docs/api-reference/chat) API to work some generative AI magic using the top search result as additional context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8abac103"
      },
      "source": [
        "## Use Chat Completions API for retrieval augmented generation\n",
        "\n",
        "\n",
        "Now we can send the question and the text to OpenAI's [Chat completions](https://platform.openai.com/docs/api-reference/chat) API.\n",
        "\n",
        "Using a LLM model together with a retrieval model is known as retrieval augmented generation (RAG). We're using Elasticsearch to do what it does best, retrieve relevant documents. Then we use the LLM to do what it does best, tasks like generating summaries and answering questions, using the retrieved documents as context.\n",
        "\n",
        "The model will generate a response to the question, using the top kNN hit as context. Use the `messages` list to shape your prompt to the model. In this example, we're using the `gpt-3.5-turbo` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1O_2PsRMTfRG"
      },
      "outputs": [],
      "source": [
        "context = response['hits']['hits'][0]['_source']['text']\n",
        "\n",
        "summary = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Answer the following question:\" + question + \"by using the following text:\" + context},\n",
        "    ]\n",
        ")\n",
        "\n",
        "choices = summary.choices\n",
        "\n",
        "for choice in choices:\n",
        "    print(\"------------------------------------------------------------\")\n",
        "    print(choice.message.content)\n",
        "    print(\"------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqkRsUpITfRG"
      },
      "source": [
        "### Code explanation\n",
        "\n",
        "Here's what that code does:\n",
        "\n",
        "- Uses OpenAI's model to generate a response\n",
        "- Sends a conversation containing a system message and a user message to the model\n",
        "- The system message sets the assistant's role as \"helpful assistant\"\n",
        "- The user message contains a question as specified in the original kNN query and some input text\n",
        "- The response from the model is stored in the `summary.choices` variable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gen AI Application\n",
        "\n",
        "In the following section, you will build a simple search application using streamlit.\n",
        "\n",
        "In this application, you can ask a question, the documents relevant to that question will be retrieved in Elasticsearch and OpenAI is used to summarize the answer using the best matching document."
      ],
      "metadata": {
        "id": "rnJbAQdgbXSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "LnL-wOdRct5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create application"
      ],
      "metadata": {
        "id": "LkEHb4VMevcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "import openai\n",
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "\n",
        "# Elastic Cloud\n",
        "es_cloud_id = os.environ['es_cloud_id']\n",
        "es_password = os.environ['es_password']\n",
        "\n",
        "# OpenAI\n",
        "openai.api_key = os.environ['openai_api_key']\n",
        "\n",
        "# Define model\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "\n",
        "# Connect to Elasticsearch\n",
        "client = Elasticsearch(\n",
        "  cloud_id = es_cloud_id,\n",
        "  basic_auth=(\"elastic\", es_password) # Alternatively use `api_key` instead of `basic_auth`\n",
        ")\n",
        "\n",
        "def openai_summarize(query, response):\n",
        "    context = response['hits']['hits'][0]['_source']['text']\n",
        "    summary = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Answer the following question:\" + query + \"by using the following text: \" + context},\n",
        "        ]\n",
        "    )\n",
        "    return summary.choices[0].message.content\n",
        "\n",
        "\n",
        "def search_es(query):\n",
        "    # Create embedding\n",
        "    question_embedding = openai.Embedding.create(input=query, model=EMBEDDING_MODEL)\n",
        "\n",
        "    # Define Elasticsearch query\n",
        "    response = client.search(\n",
        "    index = \"wikipedia_vector_index\",\n",
        "    knn={\n",
        "        \"field\": \"content_vector\",\n",
        "        \"query_vector\":  question_embedding[\"data\"][0][\"embedding\"],\n",
        "        \"k\": 10,\n",
        "        \"num_candidates\": 100\n",
        "        }\n",
        "    )\n",
        "    return response\n",
        "\n",
        "\n",
        "def main():\n",
        "    st.title(\"Gen AI Application\")\n",
        "\n",
        "    # Input for user search query\n",
        "    user_query = st.text_input(\"Enter your question:\")\n",
        "\n",
        "    if st.button(\"Search\"):\n",
        "        if user_query:\n",
        "\n",
        "            st.write(f\"Searching for: {user_query}\")\n",
        "            result = search_es(user_query)\n",
        "\n",
        "            # print(result)\n",
        "            openai_summary = openai_summarize(user_query, result)\n",
        "            st.write(f\"OpenAI Summary: {openai_summary}\")\n",
        "\n",
        "            # Display search results\n",
        "            if result['hits']['total']['value'] > 0:\n",
        "                st.write(\"Search Results:\")\n",
        "                for hit in result['hits']['hits']:\n",
        "                    st.write(hit['_source']['title'])\n",
        "                    st.write(hit['_source']['text'])\n",
        "            else:\n",
        "                st.write(\"No results found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "_J7keMUAewy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run app\n",
        "\n",
        "Run the application and check your IP for the tunneling"
      ],
      "metadata": {
        "id": "BU1WKBVGe5ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &> /content/app.log & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "-oQa-VV6e40J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the tunnel\n",
        "\n",
        "Run the tunnel and use the link below to connect to the tunnel.\n",
        "\n",
        "Use the IP from the previous step to connect to the application"
      ],
      "metadata": {
        "id": "ZXLKpEvMe-D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "ertvvtnifAZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHBfQus1TfRG"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "Now you know how to use Elasticsearch as a vector database to store embeddings, encode queries by calling the OpenAI [`embeddings`](https://platform.openai.com/docs/api-reference/embeddings) endpoint, and run semantic search using [kNN search](https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html) to find the top results.\n",
        "\n",
        "That was just one example of how to combine Elasticsearch with the power of OpenAI's models, to store embeddings, run a semantic search and enable retrieval augmented generation. RAG allows you to avoid the costly and complex process of training or fine-tuning models, by leveraging out-of-the-box models, enhanced with additional context.\n",
        "\n",
        "Use this as a blueprint for your own experiments.\n",
        "\n",
        "To adapt the conversation for different use cases, customize the system message to define the assistant's behavior or persona. Adjust the user message to specify the task, such as summarization or question answering, along with the desired format of the response."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.11.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}